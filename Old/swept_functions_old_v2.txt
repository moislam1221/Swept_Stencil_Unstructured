void seedsExpandIntoSubdomains(vector<set<int>> &territories, vector<set<int>> seeds, uint32_t * iterationLevel, uint32_t * distanceFromSeed, uint32_t * indexPtr, uint32_t * nodeNeighbors, uint32_t Ndofs, uint32_t numSubdomains, uint32_t numExpansionSteps)
{

	// Create subdomain of DOFs array to track who owns which dof
	uint32_t * subdomainOfDOFs = new uint32_t[Ndofs];
	for (int i = 0; i < Ndofs; i++) {
		subdomainOfDOFs[i] = UINT32_MAX;
		distanceFromSeed[i] = UINT32_MAX;
	}

	// Copy the seeds into the territories vector
	vector<vector<set<int>>> territoriesLevels;
	vector<set<int>> dummyVector;
	for (int i = 0; i < numSubdomains; i++) {
		dummyVector.push_back(seeds[i]);
		territoriesLevels.push_back(dummyVector);
		dummyVector.clear();
		for (auto seedDOF : seeds[i]) {
			subdomainOfDOFs[seedDOF] = i;
			distanceFromSeed[seedDOF] = 0;
		}
	}
	
	// Perform an expansion step for each subdomain, starting from the seed
	int neighbor;
	set<int> setOfNeighborsToAdd;
	for (int iter = 0; iter < numExpansionSteps; iter++) {
		for (int i = 0; i < numSubdomains; i++) {
			for (int level = 0; level < iter+1; level++) {
				for (auto seedDOF : territoriesLevels[i][level]) {
					for (int j = indexPtr[seedDOF]; j < indexPtr[seedDOF+1]; j++) {
						neighbor = nodeNeighbors[j];
						if (subdomainOfDOFs[neighbor] == UINT32_MAX && iterationLevel[neighbor] < iter+1-level) {
							setOfNeighborsToAdd.insert(neighbor);
							subdomainOfDOFs[neighbor] = i;
							distanceFromSeed[neighbor] = level+1;
							// printf("Subdomain %d, Level %d: Need to add neighbor = %d\n", i, level, neighbor);
						}
					}
				}
				if (level == iter) {
					territoriesLevels[i].push_back(setOfNeighborsToAdd);
				}
				else {		
					territoriesLevels[i][level].insert(setOfNeighborsToAdd.begin(), setOfNeighborsToAdd.end());
				}
				setOfNeighborsToAdd.clear();
			}
		}
	}

	// Concatenate all the contents of each subdomain's territories into a single set
	set<int> mergedSetInSubdomain;
	for (int i = 0; i < numSubdomains; i++) {
		mergedSetInSubdomain.clear();
		for (auto seedSet : territoriesLevels[i]) {
			mergedSetInSubdomain.insert(seedSet.begin(), seedSet.end());
		}
		territories.push_back(mergedSetInSubdomain);
	}

}

void seedsExpandIntoSubdomainsHi(meshPartitionForStage &partition, matrixInfo matrix, uint32_t * iterationLevel, uint32_t numExpansionSteps)
{
	// Create subdomain of DOFs array to track who owns which dof
	uint32_t * subdomainOfDOFs = new uint32_t[matrix.Ndofs];
	for (int i = 0; i < matrix.Ndofs; i++) {
		subdomainOfDOFs[i] = UINT32_MAX;
		partition.distanceFromSeed[i] = UINT32_MAX;
	}

	// Copy the seeds into the territories vector
	vector<vector<set<int>>> territoriesLevels;
	vector<set<int>> dummyVector;
	for (int i = 0; i < partition.numSubdomains; i++) {
		dummyVector.push_back(partition.seeds[i]);
		territoriesLevels.push_back(dummyVector);
		dummyVector.clear();
		for (auto seedDOF : partition.seeds[i]) {
			subdomainOfDOFs[seedDOF] = i;
			partition.distanceFromSeed[seedDOF] = 0;
		}
	}

	// Perform an expansion step for each subdomain, starting from the seed
	int neighbor;
	set<int> setOfNeighborsToAdd;
	for (int iter = 0; iter < numExpansionSteps; iter++) {
		for (int i = 0; i < partition.numSubdomains; i++) {
			for (int level = 0; level < iter+1; level++) {
				for (auto seedDOF : territoriesLevels[i][level]) {
					for (int j = matrix.indexPtr[seedDOF]; j < matrix.indexPtr[seedDOF+1]; j++) {
						neighbor = matrix.nodeNeighbors[j];
						if (subdomainOfDOFs[neighbor] == UINT32_MAX && iterationLevel[neighbor] < iter+1-level) {
							setOfNeighborsToAdd.insert(neighbor);
							subdomainOfDOFs[neighbor] = i;
							partition.distanceFromSeed[neighbor] = level+1;
							// printf("Subdomain %d, Level %d: Need to add neighbor = %d\n", i, level, neighbor);
						}
					}
				}
				if (level == iter) {
					territoriesLevels[i].push_back(setOfNeighborsToAdd);
				}
				else {		
					territoriesLevels[i][level].insert(setOfNeighborsToAdd.begin(), setOfNeighborsToAdd.end());
				}
				setOfNeighborsToAdd.clear();
			}
		}
	}

	// Concatenate all the contents of each subdomain's territories into a single set
	set<int> mergedSetInSubdomain;
	for (int i = 0; i < partition.numSubdomains; i++) {
		mergedSetInSubdomain.clear();
		for (auto seedSet : territoriesLevels[i]) {
			mergedSetInSubdomain.insert(seedSet.begin(), seedSet.end());
		}
		partition.territories.push_back(mergedSetInSubdomain);
	}

}

void expandToHaloRegions(vector<set<int>> &territoriesExpanded, vector<set<int>> &territories, uint32_t * indexPtr, uint32_t * nodeNeighbors, uint32_t numSubdomains)
{
	// Copy territories into territoriesExpanded
	for (int i = 0; i < numSubdomains; i++) {
		territoriesExpanded.push_back(territories[i]);
	}	

	// Add all the neigbors of members to create expanded set
	uint32_t neighbor;
	for (int i = 0; i < numSubdomains; i++) {
		for (auto dof : territories[i]) {
			for (int j = indexPtr[dof]; j < indexPtr[dof+1]; j++) {
				neighbor = nodeNeighbors[j];
				territoriesExpanded[i].insert(neighbor);
				// printf("Subdomain %d: Adding DOF %d\n", i, dof);
			}
		}
	}
}

void expandToHaloRegionsHi(meshPartitionForStage &partition, matrixInfo matrix)
{
	// Copy territories into territoriesExpanded
	for (int i = 0; i < partition.numSubdomains; i++) {
		partition.territoriesExpanded.push_back(partition.territories[i]);
	}	

	// Add all the neigbors of members to create expanded set
	uint32_t neighbor;
	for (int i = 0; i < partition.numSubdomains; i++) {
		for (auto dof : partition.territories[i]) {
			for (int j = matrix.indexPtr[dof]; j < matrix.indexPtr[dof+1]; j++) {
				neighbor = matrix.nodeNeighbors[j];
				partition.territoriesExpanded[i].insert(neighbor);
				// printf("Subdomain %d: Adding DOF %d\n", i, dof);
			}
		}
	}

}

__global__
void orderSolutionVectorBySubdomain(float * solutionOrdered, float * solution, uint32_t * territoryDOFs, uint32_t Ndofs)
{
	// Initialize index related parameters
	uint32_t i = threadIdx.x + blockIdx.x * blockDim.x;
	uint32_t index;

	// Create an ordered solution which follows the same ordering of DOFs based on subdomain
	if (i < Ndofs) {
		index = territoryDOFs[i];
		solutionOrdered[i] = solution[index];
		// printf("solutionOrdered[%d] = %f with index %d\n", i, solutionOrdered[i], index);
	}	
}

__global__
void undoOrderSolutionVectorBySubdomain(float * solution, float * solutionBySubdomain, uint32_t * territoryDOFs, uint32_t * territoryIndexPtr, uint32_t * territoryDOFsExpanded, uint32_t * territoryIndexPtrExpanded, uint32_t Ndofs)
{
	// Initialize index related parameters
	uint32_t i = threadIdx.x;
	uint32_t idx, index;
	bool update;
	uint32_t idx_lower_inner = territoryIndexPtr[blockIdx.x];
	uint32_t idx_upper_inner = territoryIndexPtr[blockIdx.x + 1];
	uint32_t idx_lower = territoryIndexPtrExpanded[blockIdx.x];
	uint32_t idx_upper = territoryIndexPtrExpanded[blockIdx.x + 1];

	// Create an ordered solution which follows the same ordering of DOFs based on subdomain
	if (i < idx_upper - idx_lower) {
		idx = i + idx_lower;
		index = territoryDOFsExpanded[idx];
		// printf("Subdomain %d: solutionBySubdomain[%d] = %f\n", blockIdx.x, i, solutionBySubdomain[idx]);
		update = false;
		for (int j = idx_lower_inner; j < idx_upper_inner; j++) {
			if (territoryDOFsExpanded[idx] == territoryDOFs[j]) {
				update = true;
				j = idx_upper_inner; // end the for loop
			}
		}
		if (update == true) {
			// printf("Subdomain %d: Thread %d, Update dof %d. The value of %f is put into solution at %d\n", blockIdx.x, i, index, solutionBySubdomain[idx], index);
			solution[index] = solutionBySubdomain[idx];
		}
	}
}

__global__
void upperPyramidalAdvance(float * solution, uint32_t * territoryDOFs, uint32_t * territoryIndexPtr, uint32_t * iterationLevels, uint32_t * distanceFromSeed, uint32_t * indexPtr, uint32_t * nodeNeighbors, uint32_t Ndofs, uint32_t numExpansionSteps)
{
	extern __shared__ float sharedMemorySolution[];

	// 1 - Move solution from global memory to shared memory
	uint32_t idx_lower, idx_upper;
	idx_lower = territoryIndexPtr[blockIdx.x];
	idx_upper = territoryIndexPtr[blockIdx.x + 1];
	if (threadIdx.x == 0) {
		// printf("{%d, %d)\n", idx_lower, idx_upper);
	}
	// for (int i = threadIdx.x; i < idx_upper - idx_lower; i += blockDim.x) {
	uint32_t i = threadIdx.x;
	if (i < idx_upper - idx_lower) {
		sharedMemorySolution[i] = solution[i + idx_lower];
		// printf("Block %d, sharedMemory[%d] corresponding to %d = %f\n", blockIdx.x, i, territoryDOFs[i+idx_lower], sharedMemorySolution[i]);
	}
	__syncthreads();
	
	// 2 - Perform updates within shared memory
	uint32_t idx, dof, neighbor, myGroundLevel;
	bool updateDOF, isMember;
	if (i < idx_upper - idx_lower) {
		for (int iter = 0; iter < numExpansionSteps; iter++) {
			if (i == 0) {
				// printf("Iter = %d\n", iter);
			}
			idx = threadIdx.x + idx_lower;
			dof = territoryDOFs[idx];
			// printf("Subdomain %d: Check dof %d\n", blockIdx.x, dof);
			myGroundLevel = iterationLevels[dof];
			updateDOF = true;
			printf("%d, %d\n", indexPtr[dof], indexPtr[dof+1]);
			for (int j = indexPtr[dof]; j < indexPtr[dof+1]; j++) {
				neighbor = nodeNeighbors[j];
				// if neighbor is not a member of all dofs in the set, duplicate DOF IS false
				isMember = false;
				for (int k = idx_lower; k < idx_upper; k++) {
					if (neighbor == territoryDOFs[k]) {
						isMember = true;
						printf("dof %d has neighbor %d\n", dof, neighbor);
					}
				}
				if (isMember == false) {
					updateDOF = false;
					// printf("Here for dof %d and neighbor %d\n", dof, neighbor);
				}
			}
			__syncthreads();
			if (updateDOF == true) {
				if (distanceFromSeed[dof] != UINT32_MAX) {
				// printf("For dof %d: The seed distance is %d and myGroundLevel is %d\n", dof, distanceFromSeed[dof], myGroundLevel);
					// printf("Not updated: In iter %d, updating %d with seed distance %d. The bound was %d and our my ground is %d\n", iter, dof, distanceFromSeed[dof], (int)(iter+1-distanceFromSeed[dof]), myGroundLevel);
				}
				else {
					// printf("We should not be updated: In iter %d, updating %d with seed distance %d. The bound was %d and our my ground is %d\n", iter, dof, distanceFromSeed[dof], (int)(iter+1-distanceFromSeed[dof]), myGroundLevel);

				}
				if ((int)myGroundLevel < (iter + 1 - (int)distanceFromSeed[dof]) && distanceFromSeed[dof] != UINT32_MAX) {
					sharedMemorySolution[i] += 1;
					// printf("In iter %d, updating %d with seed distance %d. The bound was %d and our my ground is %d\n", iter, dof, distanceFromSeed[dof], (int)(iter+1-distanceFromSeed[dof]), myGroundLevel);
					// printf("Subdomain %d: Iter %d: Updating dof %d from %f to %f at iteration level %d\n", blockIdx.x, iter, dof, sharedMemorySolution[i]-1, sharedMemorySolution[i], iterationLevels[dof]);
					iterationLevels[dof] += 1;
					printf("In iter %d, updating dof %d from %d to %d by thread, block (%d, %d) and idx_l, idx_u are %d %d with seed distance %d\n", iter, dof, iterationLevels[dof]-1, iterationLevels[dof], i, blockIdx.x, idx_lower, idx_upper, distanceFromSeed[dof]);
				}
			} 
		__syncthreads();
		}
		__syncthreads();
	}
	__syncthreads();

	// 3 - Return solution from shared memory to global memory
	// for (int i = threadIdx.x; i < idx_upper - idx_lower; i += blockDim.x) {
	if (i < idx_upper - idx_lower) {
		solution[i + idx_lower] = sharedMemorySolution[i];
	}
	__syncthreads();

}

__global__
void upperPyramidalAdvanceOld(float * solution, uint32_t * territoryDOFs, uint32_t * territoryIndexPtr, uint32_t * iterationLevels, uint32_t * indexPtr, uint32_t * nodeNeighbors, uint32_t Ndofs, uint32_t numExpansionSteps)
{
	extern __shared__ float sharedMemorySolution[];

	// 1 - Move solution from global memory to shared memory
	uint32_t idx_lower, idx_upper;
	idx_lower = territoryIndexPtr[blockIdx.x];
	idx_upper = territoryIndexPtr[blockIdx.x + 1];
	// for (int i = threadIdx.x; i < idx_upper - idx_lower; i += blockDim.x) {
	uint32_t i = threadIdx.x;
	if (i < idx_upper - idx_lower) {
		sharedMemorySolution[i] = solution[i + idx_lower];
		// printf("Block %d, sharedMemory[%d] corresponding to %d = %f\n", blockIdx.x, i, territoryDOFs[i+idx_lower], sharedMemorySolution[i]);
	}
	__syncthreads();
	
	// 2 - Perform updates within shared memory
	uint32_t idx, dof, neighbor, myGroundLevel;
	bool updateDOF, isMember;
	if (i < idx_upper - idx_lower) {
		for (int iter = 0; iter < numExpansionSteps; iter++) {
			if (i == 0) {
				printf("Iter = %d\n", iter);
			}
			idx = threadIdx.x + idx_lower;
			dof = territoryDOFs[idx];
			myGroundLevel = iterationLevels[dof];
			updateDOF = true;
			for (int j = indexPtr[dof]; j < indexPtr[dof+1]; j++) {
				neighbor = nodeNeighbors[j];
				// if neighbor is not a member of all dofs in the set, duplicate DOF IS false
				isMember = false;
				
				for (int k = idx_lower; k < idx_upper; k++) {
					if (neighbor == territoryDOFs[k]) {
						if (myGroundLevel <= iterationLevels[neighbor]) {
							isMember = true;
							// printf("Iter %d: For dof %d, neighbor %d is valid\n", iter, dof, neighbor);
						}
					}
				}
				if (isMember == false) {
					updateDOF = false;
				}
			}
			__syncthreads();
			if (updateDOF == true) {
				sharedMemorySolution[i] += 1;
				printf("In iter %d, updating %d\n", iter, dof);
				// printf("Subdomain %d: Iter %d: Updating dof %d from %f to %f at iteration level %d\n", blockIdx.x, iter, dof, sharedMemorySolution[i]-1, sharedMemorySolution[i], iterationLevels[dof]);
				iterationLevels[dof] += 1;
			} 
		__syncthreads();
		}
	}

	// 3 - Return solution from shared memory to global memory
	// for (int i = threadIdx.x; i < idx_upper - idx_lower; i += blockDim.x) {
	if (i < idx_upper - idx_lower) {
		solution[i + idx_lower] = sharedMemorySolution[i];
	}
	__syncthreads();

}


void upperPyramidalNew(float * solutionGPU, uint32_t * territoryDOFsGPU, uint32_t * territoryIndexPtrGPU, uint32_t * territoryDOFsExpandedGPU, uint32_t * territoryIndexPtrExpandedGPU, uint32_t * iterationLevelsGPU, uint32_t * distanceFromSeedGPU, uint32_t * indexPtrGPU, uint32_t * nodeNeighborsGPU, uint32_t Ndofs, uint32_t numSubdomains, uint32_t numExpansionSteps, uint32_t numElemsExpanded)
{
	// Call function to reorder the solution vector in preparation for transfer to shared memory
    float * solutionBySubdomainGPU;
    cudaMalloc(&solutionBySubdomainGPU, sizeof(float) * numElemsExpanded);
	uint32_t threadsPerBlock = 32;
	uint32_t numBlocks = ceil((float)numElemsExpanded / threadsPerBlock);

	orderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solutionBySubdomainGPU, solutionGPU, territoryDOFsExpandedGPU, numElemsExpanded);

    // Call global function to perform the upper pyramidal update
	numBlocks = numSubdomains;
	upperPyramidalAdvance<<<numBlocks, threadsPerBlock, 2 * sizeof(float) * (Ndofs/4)>>>(solutionBySubdomainGPU, territoryDOFsExpandedGPU, territoryIndexPtrExpandedGPU, iterationLevelsGPU, distanceFromSeedGPU, indexPtrGPU, nodeNeighborsGPU, Ndofs, numExpansionSteps);
	// upperPyramidalAdvance<<<numBlocks, threadsPerBlock, sizeof(float) * (Ndofs/4)>>>(solutionBySubdomainGPU, territoryDOFsExpandedGPU, territoryIndexPtrExpandedGPU, iterationLevelsGPU, indexPtrGPU, nodeNeighborsGPU, Ndofs, numExpansionSteps);

	// Back to global solution ordering	
	undoOrderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solutionGPU, solutionBySubdomainGPU, territoryDOFsGPU, territoryIndexPtrGPU, territoryDOFsExpandedGPU, territoryIndexPtrExpandedGPU, Ndofs);

}

void upperPyramidal(float * solutionGPU, uint32_t * territoryDOFsGPU, uint32_t * territoryIndexPtrGPU, uint32_t * iterationLevelsGPU, uint32_t * indexPtrGPU, uint32_t * nodeNeighborsGPU, uint32_t Ndofs, uint32_t numSubdomains, uint32_t numExpansionSteps)
{
	// Call function to reorder the solution vector in preparation for transfer to shared memory
    float * solutionOrderedGPU;
    cudaMalloc(&solutionOrderedGPU, sizeof(float) * Ndofs);
	uint32_t threadsPerBlock = 32;
	uint32_t numBlocks = ceil((float)Ndofs / threadsPerBlock);

	orderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solutionOrderedGPU, solutionGPU, territoryDOFsGPU, Ndofs);
	
    // Call global function to perform the upper pyramidal update
	numBlocks = numSubdomains;
	// upperPyramidalAdvance<<<numBlocks, threadsPerBlock, sizeof(float) * (Ndofs/4)>>>(solutionOrderedGPU, territoryDOFsGPU, territoryIndexPtrGPU, iterationLevelsGPU, indexPtrGPU, nodeNeighborsGPU, Ndofs);

	// Back to global solution ordering	
	// undoOrderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solutionGPU, solutionOrderedGPU, territoryDOFsGPU, Ndofs);

}

void advanceFunction(float * solution_d, uint32_t * iterationLevel_d, meshPartitionForStage &partition, matrixInfo matrix, uint32_t numExpansionSteps)
{
	// Call function to reorder the solution vector in preparation for transfer to shared memory
	uint32_t numElemsExpanded = partition.territoryIndexPtrExpanded[partition.numSubdomains];
    float * solutionBySubdomain_d;
    cudaMalloc(&solutionBySubdomain_d, sizeof(float) * numElemsExpanded);
	uint32_t threadsPerBlock = 32;
	uint32_t numBlocks = ceil((float)numElemsExpanded / threadsPerBlock);

	// Initialize GPU variables
	uint32_t numSubdomains = partition.numSubdomains;
	uint32_t numElems = partition.territoryIndexPtr[numSubdomains];
    uint32_t * distanceFromSeed_d, * territoryDOFs_d, * territoryDOFsExpanded_d, * territoryIndexPtr_d, * territoryIndexPtrExpanded_d;
	cudaMalloc(&distanceFromSeed_d, sizeof(uint32_t) * matrix.Ndofs);
	cudaMalloc(&territoryDOFs_d, sizeof(uint32_t) * numElems);
	cudaMalloc(&territoryDOFsExpanded_d, sizeof(uint32_t) * numElemsExpanded);
	cudaMalloc(&territoryIndexPtr_d, sizeof(uint32_t) * (numSubdomains+1));
	cudaMalloc(&territoryIndexPtrExpanded_d, sizeof(uint32_t) * (numSubdomains+1));
	cudaMemcpy(distanceFromSeed_d, partition.distanceFromSeed, sizeof(uint32_t) * matrix.Ndofs, cudaMemcpyHostToDevice);
	cudaMemcpy(territoryDOFs_d, partition.territoryDOFs, sizeof(uint32_t) * numElems, cudaMemcpyHostToDevice);	
	cudaMemcpy(territoryDOFsExpanded_d, partition.territoryDOFsExpanded, sizeof(uint32_t) * numElemsExpanded, cudaMemcpyHostToDevice);	
	cudaMemcpy(territoryIndexPtr_d, partition.territoryIndexPtr, sizeof(uint32_t) * (numSubdomains+1), cudaMemcpyHostToDevice);
	cudaMemcpy(territoryIndexPtrExpanded_d, partition.territoryIndexPtrExpanded, sizeof(uint32_t) * (numSubdomains+1), cudaMemcpyHostToDevice);
    uint32_t *indexPtr_d, *nodeNeighbors_d;
	cudaMalloc(&indexPtr_d, sizeof(uint32_t) * (matrix.Ndofs+1));
	cudaMalloc(&nodeNeighbors_d, sizeof(uint32_t) * matrix.numEntries);
	cudaMemcpy(indexPtr_d, matrix.indexPtr, sizeof(uint32_t) * (matrix.Ndofs+1), cudaMemcpyHostToDevice);
	for (int i = 0; i < matrix.Ndofs; i++) {
		printf("indexPtr[%d] = %d\n", i, matrix.indexPtr[i]);
	}
	cudaDeviceSynchronize();
	printDeviceSolutionInt(indexPtr_d, matrix.Ndofs, 6);
	cudaMemcpy(nodeNeighbors_d, matrix.nodeNeighbors, sizeof(uint32_t) * matrix.numEntries, cudaMemcpyHostToDevice);
	printDeviceSolutionInt(nodeNeighbors_d, matrix.Ndofs, 6);

	cudaDeviceSynchronize();
	orderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solutionBySubdomain_d, solution_d, territoryDOFsExpanded_d, numElemsExpanded);

	cudaDeviceSynchronize();
    // Call global function to perform the upper pyramidal update
	numBlocks = numSubdomains;
	upperPyramidalAdvance<<<numBlocks, threadsPerBlock, 4 * sizeof(float) * (matrix.Ndofs/4)>>>(solutionBySubdomain_d, territoryDOFsExpanded_d, territoryIndexPtrExpanded_d, iterationLevel_d, distanceFromSeed_d, indexPtr_d, nodeNeighbors_d, matrix.Ndofs, numExpansionSteps);

	cudaDeviceSynchronize();
	// Back to global solution ordering	
	undoOrderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solution_d, solutionBySubdomain_d, territoryDOFs_d, territoryIndexPtr_d, territoryDOFsExpanded_d, territoryIndexPtrExpanded_d, matrix.Ndofs);
	cudaDeviceSynchronize();


	cudaFree(distanceFromSeed_d);
	cudaFree(territoryDOFs_d);
	cudaFree(territoryDOFsExpanded_d);
	cudaFree(territoryIndexPtr_d);
	cudaFree(territoryIndexPtrExpanded_d);

}


void advanceNew(float * solution_d, uint32_t * iterationLevel_d, meshPartitionForStage &partition, matrixInfo matrix, uint32_t numExpansionSteps)
{
	// Call function to reorder the solution vector in preparation for transfer to shared memory
    uint32_t numElemsExpanded = partition.territoryIndexPtrExpanded[partition.numSubdomains];
	float * solutionBySubdomain_d;
    cudaMalloc(&solutionBySubdomain_d, sizeof(float) * numElemsExpanded);
	
	// GPU Kernel parameters
	uint32_t threadsPerBlock = 32;
	uint32_t numBlocks = ceil((float)numElemsExpanded / threadsPerBlock);

	// Initialize GPU variables
	uint32_t numSubdomains = partition.numSubdomains;
	uint32_t numElems = partition.territoryIndexPtr[numSubdomains];
    uint32_t * distanceFromSeed_d, * territoryDOFs_d, * territoryDOFsExpanded_d, * territoryIndexPtr_d, * territoryIndexPtrExpanded_d;
	cudaMalloc(&distanceFromSeed_d, sizeof(uint32_t) * matrix.Ndofs);
	cudaMalloc(&territoryDOFs_d, sizeof(uint32_t) * numElems);
	cudaMalloc(&territoryDOFsExpanded_d, sizeof(uint32_t) * numElemsExpanded);
	cudaMalloc(&territoryIndexPtr_d, sizeof(uint32_t) * (numSubdomains+1));
	cudaMalloc(&territoryIndexPtrExpanded_d, sizeof(uint32_t) * (numSubdomains+1));
	cudaMemcpy(distanceFromSeed_d, partition.distanceFromSeed, sizeof(uint32_t) * matrix.Ndofs, cudaMemcpyHostToDevice);
	cudaMemcpy(territoryDOFs_d, partition.territoryDOFs, sizeof(uint32_t) * numElems, cudaMemcpyHostToDevice);	
	cudaMemcpy(territoryDOFsExpanded_d, partition.territoryDOFsExpanded, sizeof(uint32_t) * numElemsExpanded, cudaMemcpyHostToDevice);	
	cudaMemcpy(territoryIndexPtr_d, partition.territoryIndexPtr, sizeof(uint32_t) * (numSubdomains+1), cudaMemcpyHostToDevice);
	cudaMemcpy(territoryIndexPtrExpanded_d, partition.territoryIndexPtrExpanded, sizeof(uint32_t) * (numSubdomains+1), cudaMemcpyHostToDevice);
    uint32_t *indexPtr_d, *nodeNeighbors_d;
	cudaMalloc(&indexPtr_d, sizeof(uint32_t) * matrix.Ndofs+1);
	cudaMalloc(&nodeNeighbors_d, sizeof(uint32_t) * matrix.numEntries);
	cudaMemcpy(indexPtr_d, matrix.indexPtr, sizeof(uint32_t) * (matrix.Ndofs+1), cudaMemcpyHostToDevice);
	cudaMemcpy(nodeNeighbors_d, matrix.nodeNeighbors, sizeof(uint32_t) * matrix.numEntries, cudaMemcpyHostToDevice);

	orderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solutionBySubdomain_d, solution_d, territoryDOFsExpanded_d, matrix.Ndofs);
	numBlocks = partition.numSubdomains;
	upperPyramidalAdvance<<<numBlocks, threadsPerBlock, 2 * sizeof(float) * matrix.Ndofs/4>>>(solution_d, territoryDOFsExpanded_d, territoryIndexPtrExpanded_d, iterationLevel_d, distanceFromSeed_d, indexPtr_d, nodeNeighbors_d, matrix.Ndofs, numExpansionSteps);
	cudaError err = cudaDeviceSynchronize();
    if( cudaSuccess != err )
    {
        printf("Failed\n");
    }
	// undoOrderSolutionVectorBySubdomain<<<numBlocks, threadsPerBlock>>>(solution_d, solutionBySubdomain_d, territoryDOFs_d, territoryIndexPtr_d, territoryDOFsExpanded_d, territoryIndexPtrExpanded_d, matrix.Ndofs);

}
